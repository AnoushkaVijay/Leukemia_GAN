{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnoushkaVijay/Leukemia_GAN/blob/main/Test_Image_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34ZKAMBjuPID"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model\n",
        "import pickle\n",
        "\n",
        "model_name = \"\"\n",
        "file_name = open(model_name, 'rb')\n",
        "final_model = pickle.load(file_name)\n",
        "print(\"Model is loaded\")"
      ],
      "metadata": {
        "id": "e4fch3lfuYki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#read the data file\n",
        "test_data = pd.read_csv(\"\")\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "y3EEYnZGvupZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label count\n",
        "test_data[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "w6ImLXKi0Qjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#labels encoding\n",
        "#first saved labels for confusion matrix axis\n",
        "labels = test_data[\"label\"].unique()\n",
        "labels.sort()\n",
        "print(labels)\n",
        "\n",
        "#then encode the labels\n",
        "test_data['label'] = pd.factorize(test_data['label'], sort = True)[0]"
      ],
      "metadata": {
        "id": "zTPI2lRev9EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#separating the true labels for the evaluations\n",
        "y_true = np.array(test_data['label'].to_list())"
      ],
      "metadata": {
        "id": "0zy194oywZVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting predictions data\n",
        "test_data_list = test_data.iloc[:,:-1].values\n",
        "\n",
        "y_pred = []\n",
        "\n",
        "for row_data in test_data_list:\n",
        "  predictions = final_model.predict(np.array([row_data]))\n",
        "  y_pred.append(predictions[0])\n",
        "\n",
        "print(\"Predictions: {}\".format(y_pred))"
      ],
      "metadata": {
        "id": "1mKq8leWwlWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation function\n",
        "'''\n",
        "def model_evaluations(y_true, y_pred, labels):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "  acc_score = accuracy_score(y_true, y_pred)\n",
        "  print(\"Accuracy score: {}\\n\".format(acc_score))\n",
        "\n",
        "  print(\"Classification Report: {}\".format(classification_report(y_true, y_pred)))\n",
        "\n",
        "  plt.figure(figsize = (10,10))\n",
        "  sns.heatmap(confusion_matrix(y_true, y_pred),  annot = True, fmt=\"g\", cmap = \"Blues\", xticklabels = labels, yticklabels = labels)\n",
        "  plt.title(\"Consfuion Matrix\")\n",
        "  plt.show()\n",
        "\n",
        "  return classification_report(y_true, y_pred, output_dict = True)\n",
        "'''"
      ],
      "metadata": {
        "id": "IaDn0wzHxHnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Function | Confusion Matrix in Percentages"
      ],
      "metadata": {
        "id": "PMDbeUDiPlk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluations(y_true, y_pred, labels):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "  acc_score = accuracy_score(y_true, y_pred)\n",
        "  print(\"Accuracy score: {}\\n\".format(acc_score))\n",
        "\n",
        "  print(\"Classification Report: {}\".format(classification_report(y_true, y_pred)))\n",
        "\n",
        "  # Compute normalized confusion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "\n",
        "    # Convert to percentages\n",
        "  cm_percentage = cm * 100\n",
        "\n",
        "    # Create annotations for confusion matrix values with the percentage sign\n",
        "  annotations = np.array([[f'{val:.2f}%' for val in row] for row in cm_percentage])\n",
        "\n",
        "  plt.figure(figsize = (10,10))\n",
        "  sns.heatmap(cm_percentage,  annot = annotations, fmt=\"\", cmap = \"Blues\", xticklabels = labels, yticklabels = labels)\n",
        "  plt.title(\"Consfuion Matrix\")\n",
        "  plt.show()\n",
        "\n",
        "  return classification_report(y_true, y_pred, output_dict = True)"
      ],
      "metadata": {
        "id": "8jSIi6htPhbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting evaulations\n",
        "report = model_evaluations(y_true, y_pred, labels)"
      ],
      "metadata": {
        "id": "UTr32FfKxNnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save results to a dataframe"
      ],
      "metadata": {
        "id": "p9zZe34BAIPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save the report as dataframe and set the indexes\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "labels_list = list(labels)\n",
        "labels_list.extend([\"accuracy\",\"macro avg\", \"weighted avg\"])\n",
        "print(labels_list)"
      ],
      "metadata": {
        "id": "vFmojBQ75sDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the dataframe\n",
        "report_df[\"Labels\"] = labels_list\n",
        "report_df.set_index(\"Labels\")\n",
        "report_df.to_csv(\"final_report.csv\", index = False)"
      ],
      "metadata": {
        "id": "VxgrxJwR7wDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}